{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Aslan x Cohere PyTorch Transformer Challenge",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aslanbakirov/AlgorithmVisualizer/blob/gh-pages/Aslan_x_Cohere_PyTorch_Transformer_Challenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UU89jyHGRT4Y"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import torch\n",
        "import math\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn.parameter import Parameter\n",
        "\n",
        "# Reference paper: Attention is All You Need! https://arxiv.org/abs/1706.03762\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Helper functions, don't worry about these.**"
      ],
      "metadata": {
        "id": "zDQm69vsJE5s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT2Config(object):\n",
        "    def __init__(\n",
        "            self,\n",
        "            vocab_size_or_config_json_file=50257,\n",
        "            n_positions=1024,\n",
        "            n_ctx=1024,\n",
        "            n_embd=768,\n",
        "            n_layer=6,\n",
        "            n_head=4,\n",
        "            layer_norm_epsilon=1e-5,\n",
        "            initializer_range=0.02,\n",
        "    ):\n",
        "        self.vocab_size = vocab_size_or_config_json_file\n",
        "        self.n_ctx = n_ctx\n",
        "        self.n_positions = n_positions\n",
        "        self.n_embd = n_embd\n",
        "        self.n_layer = n_layer\n",
        "        self.n_head = n_head\n",
        "        self.layer_norm_epsilon = layer_norm_epsilon\n",
        "        self.initializer_range = initializer_range\n",
        "\n",
        "def gelu(x):\n",
        "    return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, hidden_size, eps=1e-12):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
        "        self.bias = nn.Parameter(torch.zeros(hidden_size))\n",
        "        self.variance_epsilon = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        u = x.mean(-1, keepdim=True)\n",
        "        s = (x - u).pow(2).mean(-1, keepdim=True)\n",
        "        x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n",
        "        return self.weight * x + self.bias\n",
        "\n",
        "class Conv1D(nn.Module):\n",
        "    def __init__(self, nf, nx):\n",
        "        super(Conv1D, self).__init__()\n",
        "        self.nf = nf\n",
        "        w = torch.empty(nx, nf)\n",
        "        nn.init.normal_(w, std=0.02)\n",
        "        self.weight = Parameter(w)\n",
        "        self.bias = Parameter(torch.zeros(nf))\n",
        "\n",
        "    def forward(self, x):\n",
        "        size_out = x.size()[:-1] + (self.nf,)\n",
        "        x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)\n",
        "        x = x.view(*size_out)\n",
        "        return x\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, n_state, config):  # in MLP: n_state=3072 (4 * n_embd)\n",
        "        super(MLP, self).__init__()\n",
        "        nx = config.n_embd\n",
        "        self.c_fc = Conv1D(n_state, nx)\n",
        "        self.c_proj = Conv1D(nx, n_state)\n",
        "        self.act = gelu\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.act(self.c_fc(x))\n",
        "        h2 = self.c_proj(h)\n",
        "        return h2"
      ],
      "metadata": {
        "id": "xlhRvIX-1Wf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your implementation goes here:**"
      ],
      "metadata": {
        "id": "BBT4csH4Icd6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch.nn.functional as F\n",
        "class MultiheadAttention(nn.Module):\n",
        "    def __init__(self, embed_size, n_ctx, config):\n",
        "        super(MultiheadAttention, self).__init__()\n",
        "        assert embed_size % config.n_head == 0\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(n_ctx, n_ctx)).view(1, 1, n_ctx, n_ctx))\n",
        "        self.n_head = config.n_head\n",
        "        self.split_size = embed_size\n",
        "        self.linear_q = Conv1D(embed_size, embed_size)\n",
        "        self.linear_k = Conv1D(embed_size, embed_size)\n",
        "        self.linear_v = Conv1D(embed_size, embed_size)\n",
        "        self.c_proj = Conv1D(embed_size, embed_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        q_size, k_size, v_size= embed_size // self.n_head\n",
        "        q= self.linear_q(x) #[batch, seq_length, embed_dim] \n",
        "        k= self.linear_k(x) # [batch, seq_length, embed_dim]\n",
        "        v= self.linear_v(x) # [batch, seq_length, embed_dim]\n",
        "\n",
        "        q_head=torch.reshape(q,batch_size, seq_length, self.n_head, q_size) #. [batch, seq_length, n_head, embed_dim/n_heads]\n",
        "        k_head=torch.reshape(q,batch_size, seq_length, self.k_head, k_size) # [batch, seq_length, n_head, embed_dim/n_heads]\n",
        "        v_head=torch.reshape(q,batch_size, seq_legth, self.v_head, v_size) # [batch, seq_length, n_head, embed_dim/n_heads]\n",
        "\n",
        "        k_head= torch.transpose(k_head)  # [batch_size, n_heads, seq_length, embed_dim/n_heads]\n",
        "        scores= torch.matmul(q_head.shape, k_head) ## [batch_size, n_heads,seq_length , seq_length]\n",
        "        scores= scores//math.sqrt(q_size) ## [batch_size, n_heads, seq_length , seq_length]\n",
        "        scores=F.softmax(scores)\n",
        "        scores= torch.matmul(scores, v_head) ## [batch_size, n_heads,seq_length, embed_dim/n_heads]\n",
        "        scores= torch.cat(scores).t().contigous() ## [batch_size, seq_length, embed_dim]\n",
        "\n",
        "        return scores\n",
        "  \n",
        "class Block(nn.Module):\n",
        "  def __init__(self, n_ctx, config):\n",
        "      super(Block, self).__init__()\n",
        "      n_embed = config.n_embd\n",
        "      self.ln_1 = LayerNorm(n_embed, eps=config.layer_norm_epsilon)\n",
        "      self.attn = MultiheadAttention(n_embed, n_ctx, config)\n",
        "      self.ln_2 = LayerNorm(n_embed, eps=config.layer_norm_epsilon)\n",
        "      self.mlp = MLP(4 * n_embed, config)\n",
        "\n",
        "  def forward(self, x): # x: [batch, seq_length, embed_dim]\n",
        "      x = n_embed(x)\n",
        "      x = self.attn(x)\n",
        "      x = self.ln_1(x)\n",
        "      x= self.mlp(x)\n",
        "      x= self.ln_2(x)\n",
        "      x=F.softmax(x)\n",
        "      print(x.shape)\n",
        "      return x\n"
      ],
      "metadata": {
        "id": "Q63Em7L9Xyi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GPT2 Model**"
      ],
      "metadata": {
        "id": "5nJOS0vEI2v3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT2Model(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(GPT2Model, self).__init__()\n",
        "        self.n_layer = config.n_layer\n",
        "        self.n_embd = config.n_embd\n",
        "        self.n_vocab = config.vocab_size\n",
        "\n",
        "        self.wte = nn.Embedding(config.vocab_size, config.n_embd)\n",
        "        self.wpe = nn.Embedding(config.n_positions, config.n_embd)\n",
        "        block = Block(config.n_ctx, config)\n",
        "        self.h = nn.ModuleList([copy.deepcopy(block) for _ in range(config.n_layer)])\n",
        "        self.ln_f = LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)\n",
        "\n",
        "        embed_shape = self.wte.weight.shape\n",
        "        self.decoder = nn.Linear(embed_shape[1], embed_shape[0], bias=False)\n",
        "\n",
        "        self.loss_func = nn.CrossEntropyLoss(ignore_index=-1)\n",
        "\n",
        "\n",
        "    def forward(self, input_ids, lm_labels):\n",
        " \n",
        "        position_ids = torch.arange(0, input_ids.size(-1), dtype=torch.long,\n",
        "                                    device=input_ids.device)\n",
        "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
        "\n",
        "        input_shape = input_ids.size()\n",
        "        input_ids = input_ids.view(-1, input_ids.size(-1))\n",
        "        position_ids = position_ids.view(-1, position_ids.size(-1))\n",
        "\n",
        "        inputs_embeds = self.wte(input_ids)\n",
        "        position_embeds = self.wpe(position_ids)\n",
        "        hidden_states = inputs_embeds + position_embeds\n",
        "        for block in self.h:\n",
        "            hidden_states = block(hidden_states)\n",
        "        hidden_states = self.ln_f(hidden_states)\n",
        "        output_shape = input_shape + (hidden_states.size(-1),)\n",
        "        hidden_states = hidden_states.view(*output_shape)\n",
        "        lm_logits = self.decoder(hidden_states)\n",
        "        loss = self.loss_func(lm_logits.view(-1, lm_logits.size(-1)), lm_labels.view(-1))\n",
        "        return loss\n"
      ],
      "metadata": {
        "id": "euoadZ0vYG6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Time to test it!**\n",
        "\n",
        "**Here is a naive training loop and the loss should go down**"
      ],
      "metadata": {
        "id": "WR3oM05KH9aq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "config = GPT2Config()\n",
        "model = GPT2Model(config)\n",
        "dum_input = torch.range(1,config.n_ctx,dtype=torch.int64)\n",
        "dum_input = torch.reshape(dum_input,(1,config.n_ctx))\n",
        "\n",
        "\n",
        "#train loop\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "running_loss = 0.0\n",
        "for i in range(50):\n",
        "    # get the inputs; data is a list of [inputs, labels]\n",
        "    inputs = torch.range(1, config.n_ctx,dtype=torch.int64)\n",
        "    inputs = torch.reshape(dum_input,(1,config.n_ctx)) \n",
        "    labels = inputs + 1\n",
        "\n",
        "    # zero the parameter gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # forward + backward + optimize\n",
        "    loss = model(input_ids=inputs,lm_labels=labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print(\"step \",i ,\" loss: \", loss.item())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-R6-9kt8224f",
        "outputId": "7ea9b450-fc92-4732-c699-1d38f57326e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
            "  del sys.path[0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "step  0  loss:  10.988716125488281\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "step  1  loss:  10.98796558380127\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "step  2  loss:  10.986539840698242\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "step  3  loss:  10.984503746032715\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "step  4  loss:  10.98192024230957\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "step  5  loss:  10.97884464263916\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "step  6  loss:  10.975324630737305\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "step  7  loss:  10.971405982971191\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "step  8  loss:  10.96712875366211\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "step  9  loss:  10.962528228759766\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "step  10  loss:  10.957634925842285\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "step  11  loss:  10.952482223510742\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "step  12  loss:  10.947092056274414\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "step  13  loss:  10.94149112701416\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "step  14  loss:  10.935699462890625\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "step  15  loss:  10.929734230041504\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "step  16  loss:  10.923615455627441\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "step  17  loss:  10.9173583984375\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "step  18  loss:  10.910974502563477\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "step  19  loss:  10.904479026794434\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "step  20  loss:  10.897882461547852\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "step  21  loss:  10.891192436218262\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "step  22  loss:  10.884422302246094\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "step  23  loss:  10.877577781677246\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "step  24  loss:  10.87066650390625\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "step  25  loss:  10.863696098327637\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "step  26  loss:  10.856673240661621\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "step  27  loss:  10.849599838256836\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "step  28  loss:  10.842483520507812\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "step  29  loss:  10.8353271484375\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "step  30  loss:  10.828137397766113\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "step  31  loss:  10.82091236114502\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "step  32  loss:  10.8136625289917\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "step  33  loss:  10.80638599395752\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "step  34  loss:  10.79908561706543\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "step  35  loss:  10.791763305664062\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "step  36  loss:  10.784423828125\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "step  37  loss:  10.777067184448242\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "step  38  loss:  10.769696235656738\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "step  39  loss:  10.762310981750488\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "step  40  loss:  10.754913330078125\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "step  41  loss:  10.747505187988281\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "step  42  loss:  10.740086555480957\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "step  43  loss:  10.732659339904785\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "step  44  loss:  10.72522258758545\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "step  45  loss:  10.717781066894531\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "step  46  loss:  10.710331916809082\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "step  47  loss:  10.702875137329102\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "step  48  loss:  10.695416450500488\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "torch.Size([1, 1024, 768])\n",
            "step  49  loss:  10.68795108795166\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "aS4pbukwfAa9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}